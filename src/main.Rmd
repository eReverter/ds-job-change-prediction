---
title: "Assignment 2"
author: 'Enric Reverter & Gerard Pons'
date: "14/10/2021"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, eval=F}
knitr::opts_chunk$set(echo = TRUE)
```

In this assignment, a binary model has been created to predict whether or not a candidate will work for a company, or in other words, if he or she will change jobs. The process of creating the model started from understanding and cleaning the data, which in this dataset was a challenging but very important task. Then it continued by progressively building the model, first by checking the best way to treat some variables (i.e as factor or numerical), assessing transformations, additions and interactions. After that, the residuals and influential observations were addressed, and the model was reevaluated. Finally, the predictive power of the model was assessed. The steps hereunder document this process in a more detailed way.

### Required libraries

```{r, eval=F, warning=F, message=F}
## Data manipulation
library(tidyverse)
library(dplyr)
library(mice)
library(Hmisc)
## Statistics
library(lsr)
library(missMDA)
library(VIM)
library(chemometrics)
library(arules)
library(skimr)
library(car)
library(FactoMineR)
library(factoextra)
library(effects)
## Plots
library(ggplot2)
library(ggExtra)
library(ggthemes)
library(processx)
library(plotly)
library(cowplot)
library(gridExtra)
library(RColorBrewer)
theme_set(theme_bw())
## Set data path
setwd("..")
data_path = file.path(getwd(), "data")
plot_path = file.path(getwd(), "plots")
```

# Data Exploration

Sample from the original dataset:
```{r, eval=F}
data = read.csv(file.path(data_path, "aug_train.csv"))
set.seed(020198)
sample = sample(1:nrow(data), 5000)
df = data[sample,]
write.csv(df, file.path(data_path, "jobs.csv"), row.names = FALSE)
```

Or load the dataset in case it is already stored:
```{r}
df = read.csv(file.path(data_path, "jobs.csv"))
```

Skim over it:
```{r, eval=F}
head(df)
summary(df)
str(df)
```

Convert data types to the proper format:
```{r}
df = df %>%
  mutate(across(where(is.character), ~ na_if(., ""))) %>%
  mutate(across(where(is.character) | matches("target"), ~ as.factor(.)))
```

Detail of factors:
```{r, eval=F}
df %>%
  select(., where(is.factor)) %>%
  sapply(., table)
table(df$last_new_job)
```

## Missing Values

As it can be observed, the dataset contains a lot of missing values, in some cases even exceeding the 30% of values in a given attribute. These missing values might condition the imputation methods, which is first done using logic. Then, algorithms are used. Also, there is a set of 21 observations with more than 50% of the variables (that will be used) as NA, which have been decided to be deleted from the working set:
```{r}
count_na = function(x) {sum(is.na(x))}
df = df %>%
  mutate(across(matches("company"), ~ as.character(.))) %>%
  mutate(across(matches("company"), ~ na_if(., "NA"))) %>%
  mutate(across(matches("company"), ~ as.factor(.))) %>%
  mutate(count_na = apply(., 1, count_na))
summary(df$count_na)
boxplot(df$count_na)
table(df$count_na)
```

Visualizing the missing values prior to dropping them:
```{r}
library(reshape2)
ggplot_missing <- function(data){
  df2 <- data %>% is.na %>% melt

  ggplot(df2, aes(Var2, Var1, fill=value)) + 
    geom_raster() +
    theme_minimal() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
    scale_fill_grey(name="", labels=c("Present", "Missing")) +
    theme(axis.text.x  = element_text(angle=45, vjust=1, hjust=1)) + 
    labs(title = "Number of Missing Values Across the Data",
         x = "Variables",
         y = "Observations")
}

ggplot_missing(select(df, -c("count_na")))
```

Deleting the observations with many NA's:
```{r}
df = df %>%
  filter(., count_na < 5) %>%
  select(., -c("count_na"))
```

The rules used for logically imputation are stated as follow, always assuming that everyone in the dataset is currently working, as the target is looking or not for a job change:

- If the education_level is null but they are enrolled in a university, the education is set to high school.
- If major_discipline is not null, the education level should be at least graduate.
- If company_type is known and company_size is missing, it is left for imputation and vice versa. If both are missing they are labeled as Unknown, as the number of missing values for company information exceeds 30%.
- If gender is missing, it is imputed with Unknown, as there are nearly 30% of missing values in gender.
- If major_discipline is null, it is imputed with Other if the education level is Graduate, Masters or PhD, and imputed to No Major otherwise.
- If experience , last_New_Job and company_ information are null, the experience is imputed to <1.

```{r}
df = df %>%
  mutate(f.enrolled = case_when(enrolled_university == "no_enrollment" ~ "No",
                                !is.na(enrolled_university) ~ "Yes"))
df = df %>%
  # Convert factors to strings in order to impute them
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  
  # Impute education level as mentioned above
  mutate(education_level = case_when(is.na(education_level) & f.enrolled == "Yes" ~ "High School",
                                     !is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "Graduate",
                                     TRUE ~ education_level)) %>%
  
  # Impute major_discipline as mentioned above
  mutate(major_discipline = case_when(is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "No Major",
                                      is.na(major_discipline) & education_level %in% c("Graduate", "Masters", "Phd") ~ "Other",
                                      TRUE ~ major_discipline)) %>%
  
  # Impute enrolled_university
  mutate(enrolled_university = case_when(is.na(enrolled_university) & education_level %in% c("Masters", "Phd") ~ "no_enrollment",
                                         TRUE ~ enrolled_university)) %>%
  
  # Impute experience
  mutate(experience = case_when(is.na(experience) & (is.na(last_new_job) & is.na(company_size) & is.na(company_type)) ~ "<1",
                                TRUE ~ experience)) %>%
  
  # Impute gender
  mutate(gender = case_when(is.na(gender) ~ "Other",
                            TRUE ~ gender)) %>%
  
  # Impute company
  mutate(company_size = case_when(is.na(company_size) & is.na(company_type) ~ "Unknown",
                                  TRUE ~ company_size)) %>%
  mutate(company_type = case_when(is.na(company_type) & company_size == "Unknown" ~ "Other",
                                  TRUE ~ company_type)) %>%
  
  # Convert back to factors    
  mutate(across(where(is.character), ~ as.factor(.))) %>%
  
  # Drop unused columns
  select(., -c("f.enrolled"))
```

Visualizing the missing values posterior to the logical imputation:
```{r}
ggplot_missing(df)
```

After the logical imputation, the NA values do not account for more than 2% in any of the categories, and it has been decided to impute them with factorial analysis for mixed data. It must be noted that a new flag attribute ‘Imputed’ has been created, in order to keep track of these imputed observations when modelling, as they could cause problems.

Indicator of rows which still have NA's:
```{r}
colSums(is.na(df))

imputed_indicator = function(x) {if(count_na(x)>0) {return(TRUE)} else {return(FALSE)}}

df = df %>%
  mutate(imputed = apply(., 1, imputed_indicator))
```

### FAMD Imputation

Impute with FAMD method:
```{r}
res.famd = imputeFAMD(select(df, -c("target", "city", "enrollee_id", "imputed")))
```

As it can be seen, the class frequencies after imputation have been compared to the ones before it, and there is no notable change.
```{r}
round(prop.table(table(df$education_level))*100,1)
round(prop.table(table(res.famd$completeObs$education_level))*100,1)
round(prop.table(table(df$last_new_job))*100,1)
round(prop.table(table(res.famd$completeObs$last_new_job))*100,1)
round(prop.table(table(df$enrolled_university))*100,1)
round(prop.table(table(res.famd$completeObs$enrolled_university))*100,1)
summary(df$training_hours)
```

Store the complete dataset:
```{r}
df = data.frame(res.famd$completeObs, select(df, c("target", "city", "enrollee_id", "imputed")))
```

Mutate strings after FAMD converted them into dummy variables:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(gender = str_remove(gender, "gender_")) %>%
  mutate(major_discipline = str_remove(major_discipline, "major_discipline_")) %>%
  mutate(company_type = str_remove(company_type, "company_type_")) %>%
  mutate(experience = str_remove(experience, "experience_")) %>%
  mutate(last_new_job = str_remove(last_new_job, "last_new_job_")) %>%
  mutate(across(where(is.character), ~ as.factor(.)))
```

With the complete dataset, some new attributes have been crated: a new numerical variable has been created from the factor experience and a new factor has been created from the variable of city development index. In future steps, it will be decided which one is the most suitable for the modelling process. It must be noted that since company size had a lot of NA's, it has not been converted into numerical.

Convert experience into a numerical variable:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(n.experience = case_when(experience == "<1" ~ "0",
                                experience == ">20" ~ "25",
                                TRUE ~ experience)) %>%
  mutate(n.experience = as.integer(n.experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))

summary(df$n.experience)
summary(df$experience)
```

Convert city development index into a categorical variable:
```{r}
groups = 5

df$f.city_development_index = as.ordered(cut2(df$city_development_index, g=groups, m=nrow(df)/groups))
table(df$f.city_development_index)
```

Write the dataset:
```{r, eval=F}
write.csv(df, file.path(data_path, "jobs_complete.csv"), row.names = FALSE)
```

## Outlier treatment

Univariate outliers can not be seen in the dataset for the two numerical variables. One could think that training_hours contains some outliers, as they are above the extreme threshold. However, they are not too extreme and all of them have a very plausible value, hence imputation would not be a good practice in this case.
```{r, eval=F}
extreme_out = quantile(df$training_hours)[[4]]+3*IQR(df$training_hours)

ggplot(data = df, aes(x="", y=training_hours)) +
  geom_boxplot(width=0.5) +
  geom_hline(yintercept = extreme_out, color="red") +
  scale_y_continuous(labels=scales::comma) 
labs(title='Boxplot Training Hours',
     y="Training Hours") +
  # Do not show x axis
  theme(axis.text.x=element_blank(), axis.ticks.x = element_blank(), axis.line.x = element_blank(), axis.title.x=element_blank())

num_outliers = df %>%
  filter(., training_hours > extreme_out) %>%
  nrow()
num_outliers

outliers = df %>%
  filter(., training_hours > extreme_out)

# prop.table(table(df$gender))
# prop.table(table(outliers$gender))
# prop.table(table(df$relevent_experience))
# prop.table(table(outliers$relevent_experience))
# prop.table(table(df$enrolled_university))
# prop.table(table(outliers$enrolled_university))
# prop.table(table(df$education_level))
# prop.table(table(outliers$education_level))
# prop.table(table(df$major_discipline))
# prop.table(table(outliers$major_discipline))
# prop.table(table(df$last_new_job))
# prop.table(table(outliers$last_new_job))
```

## Factor Visualizations

Let's take a look at the categorical variables with which the modelling is done.

```{r}
plist = list()
cat_vars = c("gender", "relevent_experience", "enrolled_university", "education_level", "major_discipline", "company_size", "company_type", "last_new_job")
```


```{r}
for (i in 1:(length(cat_vars))) {
  plist[[i]] = df %>%
    group_by(!!as.name(cat_vars[i]), target) %>%
    summarise(n = n()/nrow(df)) %>%
    ggplot(data=., aes(x=reorder(!!as.name(cat_vars[i]), -n), y=n, fill=target)) +
    geom_bar(position="stack", stat="identity") +
    scale_fill_brewer(palette = "Blues") +
    scale_y_continuous(limits=c(0, 1)) +
    geom_text(aes(label=sprintf("%0.2f", round(n, digits = 2))), position = position_stack(vjust = 0.5), size=2.9) +
    labs(x=cat_vars[i],
           fill="Target") +
    theme(legend.position="none") +
    theme(axis.title.y=element_blank()) + 
    scale_x_discrete(labels = function(labels) {
      fixedLabels = c()
      for (l in 1:length(labels)) {
        fixedLabels[l] = paste0(ifelse(l %% 2 == 0, '', '\n'), labels[l])
      }
      return(fixedLabels)
    })
}

p = df %>%
    group_by(!!as.name(cat_vars[i]), target) %>%
    summarise(n = n()) %>%
    ggplot(data=., aes(x=reorder(!!as.name(cat_vars[i]), -n), y=n, fill=target)) +
    geom_bar(position="stack", stat="identity") +
    scale_fill_brewer(palette = "Blues") +
    labs(x=cat_vars[i], fill="Target") +
    guides(fill = guide_legend(nrow = 1))

legend = get_legend(p + theme(legend.box.margin = margin(0, 0, 0, 12), 
                              legend.box = "horizontal", 
                              legend.title.align=0.5,
                              legend.background = element_rect(linetype="solid",
                                                               color="grey")))

title = ggdraw() + draw_label("Barplots - Categorical Variables Freq.", fontface='bold')
empty = ggdraw()
p = plot_grid(title, empty, plotlist = plist[1:4], ncol = 2, rel_heights = c(0.2,1,1))
q = plot_grid(title, empty, plotlist = plist[5:8], ncol = 2, rel_heights = c(0.2,1,1))

pp = plot_grid(p, legend, ncol = 1, rel_heights = c(1, 0.1))
qq = plot_grid(q, legend, ncol = 1, rel_heights = c(1, 0.1))

pp
qq

# ggsave(file=file.path(plot_path,"barplot_freq1.png"), plot=pp)
# ggsave(file=file.path(plot_path,"barplot_freq2.png"), plot=qq)
```

# Modelization

Before starting with the model, it is interesting to describe the response variable. It can be seen that it is significantly associated with all the numerical and categorical variables, except for training_hours, which sits really close to the threshold. It is also worth noting that the variables which have been kept in purpose both in numerical and categorical form are the ones that have a more significant association, meaning that the future assessment of how to treat them will be of particular interest. Overall, what can be said is that in general, people who want to change jobs tend to be from less developed cities with no data regarding the company, have less experience, and a higher education.
```{r}
cat = FactoMineR::catdes(df[,-c(13:15)], 12)
cat$test.chi2
cat$quanti.var
cat$category
cat$quanti
```

Before starting with the modelling, the data should be split into working and test datasets, so that the created model can be compared and assessed with data that it has not seen, hence limiting overfitting. The chosen splitting size was 75-25.
```{r}
library(caret)
set.seed(020198)
trainIndex = createDataPartition(df$target, p = 0.75, list = FALSE, times = 1)
train = df[trainIndex,]
test = df[-trainIndex,]
```

Inspect the null model:
```{r}
df = select(train, -c("city", "enrollee_id"))
m0 = glm(target ~ 1, data=df, family=binomial)
summary(m0)
```

After computing the null model, it was assessed how to treat the attribute experience: as a factor or as a numerical variable. 

Regarding being numerical, polynomial transformations were applied to it. It was seen that the p-value for a third degree polynomial suggests that this transformation is not needed (this conclusion can only be drawn because the variables constructed by Poly function are orthogonal), hence only a second order polynomial was kept. Using deviance tests, the comparison with the normal variable and the transformed one yield significantly different models, and with a better performance for the transformed one.
```{r}
mnexp = glm(target ~ n.experience, data=df, family=binomial)
summary(mnexp)
mnexppoly3 = glm(target ~ poly(n.experience,3), data=df, family=binomial)
summary(mnexppoly3)
mnexppoly2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)
summary(mnexppoly2)
anova(mnexp,mnexppoly2,test='Chisq')
```

Regarding experience as a factor, since it has more than 20 categories some collapses have been found to improve the model results:
-Collapsing by quantiles
-Collapsing the model logically in Entry Level, Junior Level, Mid Level, Senior Level and Chief Level, using some well defined year ranges for the Data Science field.
```{r}
entry_level = c('<1','1','2')
junior_level = c('3','4')
mid_level = c('5','6')
senior_level = c('7','8','9','10')
chief_level = c('11','12','13','14','15','16','17','18','19','20','>20')

df = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(collapsed_exp = case_when(experience %in% entry_level ~ "Entry Level",
                                experience %in% junior_level ~ "Junior Level",
                                experience %in% mid_level ~ "Mid Level",
                                experience %in% senior_level ~ "Senior Level",
                                experience %in% chief_level ~ "Chief Level",
                                TRUE ~ experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))


groups = 5

df$collapsed_exp2 = as.ordered(cut2(df$n.experience, g=groups, m=nrow(df)/groups))
# table(df$collapsed_exp)
# table(df$collapsed_exp2)
```

Comparing both collapsed models, it can be seen that the one collapsed by quantiles is better.
```{r} 
mcexp = glm(target ~ experience, data=df, family=binomial)
summary(mcexp)

mcexpcol = glm(target ~ collapsed_exp, data=df, family=binomial)
mcexpcol2 = glm(target ~ collapsed_exp2, data=df, family=binomial)
```

After getting the best numerical and categorical transformations for the variable, the models created with them were compared. As they are not nested models, the deviance test anova() can not be applied, and it was decided to use AIC instead. It can be clearly seen that the numerical treatment of the variable outperforms the categorical, hence is the one that will be used in the following models. 
```{r}
AIC(mcexp,mcexpcol,mnexppoly2,mnexp, mcexpcol2)
```

The same analysis can be done for the city development index (which will not be extensively reported). Even after performing the transformation suggested by the MarginalModelPlots, the discretized version of the city development index is much better.
```{r}
mncdi = glm(target ~ city_development_index, data=df, family=binomial); summary(mncdi) # Numerical
mfcdi = glm(target ~ f.city_development_index, data=df, family=binomial); summary(mfcdi) # Categorical (collapsed above)

AIC(mncdi, mfcdi)

# For the improved cdi^-0.5 - known from the marginal model plots
marginalModelPlots(mncdi)
mncdi_tr = glm(target ~ I(city_development_index^-0.5), data=df, family=binomial)
marginalModelPlots(mncdi_tr)

AIC(mncdi,mncdi_tr, mfcdi)

# Discretizing the transformed index

groups = 5

df$f.city_development_index_tr = as.ordered(cut2(df$city_development_index^-0.5, g=groups, m=nrow(df)/groups))
mfcdi_tr = glm(target ~ f.city_development_index_tr, data=df, family=binomial)
AIC(mfcdi, mfcdi_tr)
```

After having chosen the best type of variables to work with, the focus is firstly set on the two numerical variables, whose models are compared with and without interactions. As it can be seen with the deviance test, adding training hours to the model, either as an interaction or just an addition, does not yield a statistically different model, hence only the second order transformation of experience is kept.
```{r}
m1 = glm(target ~ training_hours, data=df, family=binomial)
m2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)

m3 = glm(target ~ training_hours+poly(n.experience,2), data=df, family=binomial)
m4 = glm(target ~ training_hours*poly(n.experience,2), data=df, family=binomial)

# Gross effects
anova(m0,m1,test="Chisq")
anova(m0,m2,test="Chisq")

# Net effects
anova(m1,m3,test="Chisq")
anova(m2,m3,test="Chisq")

# Interaction effects
anova(m3,m4,test="Chisq")

AIC(m0, m1, m2, m3, m4) 
```

Assessing it with marginal model plots, no transformations are suggested, as it yields a perfect fit.
```{r}
marginalModelPlots(m2) 
```

After that, the additive effect of variables is explored by using a step function with AIC, to be more permissive. It results in suggesting the addition of 7 of the variables to the model, which is significantly different and much better than the previous best one. Also, multicollinearity was discarded by doing a vif test.
```{r}
df = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "city_development_index", "f.city_development_index_tr"))
m5 = glm(target ~ poly(n.experience,2) + . - imputed, data=df, family=binomial)
maic = step(m5)
summary(maic)
vif(maic)

anova(m2,maic,test="Chisq")
AIC(m2,maic)
```

Some factors still have a lot of levels, but two of them can be further collapsed to improve the model results:
```{r}
# Company size collapse
aux = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(company_size = case_when(company_size != "Unknown" ~ "Known",
                                  TRUE ~ company_size)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))

maux = glm(formula = target ~ poly(n.experience, 2) + relevent_experience + 
    enrolled_university + education_level + company_size + last_new_job + 
    training_hours + f.city_development_index, family = binomial, 
    data = aux)

# Education level collapse
aux2 = aux %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(education_level = case_when(education_level %in% c("Masters", "Phd") ~ "MastersPhd",
                                  TRUE ~ education_level)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))

maux2 = glm(formula = target ~ poly(n.experience, 2) + relevent_experience + 
    enrolled_university + education_level + company_size + last_new_job + 
    training_hours + f.city_development_index, family = binomial, 
    data = aux2)
```

To check for interactions the step function was used again, but this time using the BIC criterion in order to be more restrictive. The BIC criterion removed almost all the possible interactions but the one between company size and city index development (factorized).
```{r}
mbic = step(maux, scope = . ~ .^2, k = log(nrow(df)))
mbic2 = step(maux2, scope = . ~ .^2, k = log(nrow(df)))

AIC(maic, maux, maux2, mbic, mbic2) # Go with mbic2
summary(mbic2)

df = aux2
mb = mbic2
```

Henceforth, the model resulting from the bic step is the one studied.

## Model Interpretation ### Explain

The model formula is as follows:

logit($\pi$~ijk~)$= \eta + \beta$~1~$experience$+$\beta$~2~$experience^2+\alpha$~i~+$\nu$~j~+$\kappa$~k~ +$\nu\kappa$~jk~

To interpret it, it has to be stated that the reference level is Graduate, Known company Size and from the quantile of cities with poorest development. 

Some of the coefficients can be interpreted as follows:
- When considering experience, all else equal, the log odds decrease during the first 20 years, and then start increasing.
- Considering education level, all else equal, having low levels of education (high/primary school) decrease considerably the log odds, compared to the reference level(graduate), and Masters or PhD decrease it slightly.
- For city development, it can be said:
  - Company_size Known: the log odds are reduced by increasing order of city development (-2,-2.38,-2.5,-2.52), all else equal. Hence the odds of changing jobs are higher for people from less developed cities from known company_size
  - Company_size Unknown: to assess this case, the value of the interaction coefficient must be added respectively. All else equal, the same conclusion as before can be reached, but in this case the decrease in log odds is much more smaller.

A similar argumentation could be done for company size.

Hence, people that live in an underdeveloped city and not have not reported working for company are more prone to change jobs, as well as people with a Graduate or Masters/PhD. 

```{r}
summary(mb)
coef(mb)
```

## Model Diagnostics

The added variable plots, which can bee seen in the Annex, do not show any alarming behavior and the same can be said for the MarginalModelPlots. Regarding the residualPlots, there is not much to be said as overall the behavior is acceptable. In the allEffects plots, it can be observed how the probabilities of wanting to change jobs decreses with experience, increases for higher levels of education, and is sligthly higher when the company information is not provided. There is a clear difference in developed countries where company information is provided or not.

```{r}
avPlots(mb)
marginalModelPlots(mb)
residualPlots(mb)
plot(allEffects(mb))
```

Studentized Residuals: Some outliers have been found in them, but they do not have,in any case, a value greater than 3. It can be seen that they are all people who want to change jobs, and contrary to the whole dataset, the vast majority of them are only high school graduates (hence no major). Also, all of them had specified the company size. Moreover, when performing an outlierTest only one observation is detected, but it will also be detected and removed when assessing the cook's distance.
```{r}
n = dim(df)[1]
p = mb$rank
res_mb = rstudent(mb)
cut_off = qt(0.995,n-p-1)

ls = Boxplot(res_mb)
abline(h=cut_off,col=2)
abline(h=-cut_off,col=2)

nrow(df[which(abs(res_mb)>cut_off),])

aux = df[which(abs(res_mb)>cut_off),]
summary(aux)

outlierTest(mb) # The outlier is already taken into account in the cooks distance

# prop.table(table(aux$gender)); prop.table(table(df$gender))
# prop.table(table(aux$relevent_experience)); prop.table(table(df$relevent_experience))
# prop.table(table(aux$enrolled_university)); prop.table(table(df$enrolled_university))
# prop.table(table(aux$education_level)); prop.table(table(df$education_level))
# prop.table(table(aux$major_discipline)); prop.table(table(df$major_discipline))
# prop.table(table(aux$company_size)); prop.table(table(df$company_size))
# prop.table(table(aux$company_type)); prop.table(table(df$company_type))
# prop.table(table(aux$last_new_job)); prop.table(table(df$last_new_job))
# prop.table(table(aux$training_hours)); prop.table(table(df$training_hours))
# prop.table(table(aux$n.experience)); prop.table(table(df$n.experience))
# prop.table(table(aux$f.city_development_index)); prop.table(table(df$f.city_development_index))
# prop.table(table(aux$target)); prop.table(table(df$target))
```

Hat Values: The cut off for this assessment has been 4 times the mean,as the dataset can be considered big enough. Regarding the description of the observations that fall under the criterion, there is a large proportion of people with no experience, with only primary school education (hence no major) and in this case all of them have not specified the company. As the had values indicate the leverage, these outliers have not been removed as the overall effect will be assessed with Cook's distance, taking into account discrepancy.
```{r}
hat = hatvalues(mb)
hat_cut = 4*p/n 

Boxplot(hat)
abline(h=hat_cut,col=2)

sum(hat>hat_cut)

aux = df[which(hat>hat_cut),]
summary(aux)
```

Cook's distance: For the Cook's distance criterion, a threshold had to be defined to match the need of our model as a group of observations can clearly be seen as outlier far from the main group of observations. As before, it can be seen that the proportions for people with no experience and with primary school education. Moreover, all of them want to change jobs. Fortunately, none of the resulting influential observations is one of the ones that were imputed in the previous steps.
```{r}
cook = cooks.distance(mb)
lc = Boxplot(cook, id=list(n=18))
cook_cut = 0.0045
nrow(df[which(cook>cook_cut),])

abline(h=cook_cut, col=2)

aux = df[which(cook>cook_cut),]
summary(aux)
```

The influential data can be clearly seen with the help of an influence plot:
```{r}
influencePlot(mb)
```

## Reevaluate the model

The outliers detected with the Cook's distance method have been removed from the dataset, and the model has been reevaluated without those observations. The two models have been evaluated firstly with AIC, knowing that it is not a strictly accurate comparison as the number of observations differs from one model to the other. Since it only differs by 7 observations some general intuition of the behaviour can be obtained. Thus, it can be seen that the new reevaluated model seems to be better, and the influencePlot results are much more accurate. 
```{r}
# 1680 %in% which(cook<cook_cut)
daux = df
df = df[which(cook<cook_cut),]

mbest = glm(formula = target ~ poly(n.experience, 2) + education_level + 
    company_size + f.city_development_index + company_size:f.city_development_index, 
    family = binomial, data = df)

AIC(mb, mbest)

influencePlot(mbest)
```

## Model Performance Evaluation

Now that the model has been improved, the ROC curve can be assessed to further diagnose its performance. The area under the curve (AUC) is computed for both models, which also serves as an indicator to compare them. First, the curve for the reevaluated model can be depicted, then the AUC's are displayed.
```{r}
library(pROC)
prob=predict(mbest, type=c("response"))
df$prob=prob
g = roc(target ~ prob, data = df)
plot(g)
```

The AUC has been assessed for the working set, where a 0.5% improvement can be seen from the reevaluated model, which supports the previous conclusion that the removing the influential observations was beneficial. Regarding its value, 81.3% can be considered a very good model, even more so considering the imbalance in the response variable.
```{r}
# Model
prob = predict(mb, type=c("response"))
auc(daux$target, prob)

# Reevaluated model
prob = predict(mbest, type=c("response"))
auc(df$target, prob)
```

Lastly, the confusion matrix is also assessed on the test set. First of all, the same transformations that have been done during the modelling steps have to be applied to the test set.
```{r}
# aux = test
test = select(test, -c("city", "enrollee_id", "imputed"))

test = test %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(n.experience = case_when(experience == "<1" ~ "0",
                                experience == ">20" ~ "25",
                                TRUE ~ experience)) %>%
  mutate(n.experience = as.integer(n.experience)) %>%
  mutate(company_size = case_when(company_size != "Unknown" ~ "Known",
                                  TRUE ~ company_size)) %>%
  mutate(education_level = case_when(education_level %in% c("Masters", "Phd") ~ "MastersPhd",
                                  TRUE ~ education_level)) %>%
  mutate(across(where(is.character), ~ as.factor(.))) %>%
  select(., -c("experience"))

table(df$f.city_development_index)
test$f.city_development_index = as.ordered(cut2(test$city_development_index, cuts = c(0.691, 0.878, 0.920, 0.921)))
table(test$f.city_development_index)
levels(test$f.city_development_index) = c('[0.448,0.691)', '[0.691,0.878)', '[0.878,0.920)','0.920', '[0.921,0.949]')
```

Then, the confusion matrix on the test set can be depicted in order to better assess the model. It can be observed how the model does not overfit the training data and it is much better than a random model (as already seen with AUC). Notice how the positive response (1) is in this case the negative one, and vice versa. As such, the specificity indicates how well it is being predicted that someone will change its job (0.5179). This is decent accounting for the fact that the target is imbalanced. The measures related to the person not wanting to change job are all good. Overall, the model has a good accuracy and balanced accuracy.
```{r}
prob = predict(mb, newdata = test, type = "response")
test$prob = ifelse(prob<0.5,0,1)
confusionMatrix(data = as.factor(test$prob), reference = test$target)
```

Predicting an outcome: 
```{r}

```


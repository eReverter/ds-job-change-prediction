---
title: "Assignment 2"
author: 'Enric Reverter & Gerard Pons'
date: "14/10/2021"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, eval=F}
knitr::opts_chunk$set(echo = TRUE)
```

Assumptions:
- Sample from incomplete dataset.
- Everyone in this dataset is currently working.


### Required libraries

```{r, eval=F}
## Data manipulation
library(tidyverse)
library(dplyr)
library(mice)
library(Hmisc)
## Statistics
library(lsr)
library(missMDA)
library(VIM)
library(chemometrics)
library(arules)
library(skimr)
library(car)
library(FactoMineR)
library(factoextra)
library(effects)
## Plots
library(ggplot2)
library(ggExtra)
library(ggthemes)
library(processx)
library(plotly)
library(cowplot)
library(gridExtra)
library(RColorBrewer)
theme_set(theme_bw())
## Set data path
setwd("..")
data_path = file.path(getwd(), "data")
plot_path = file.path(getwd(), "plots")
```


# Data Exploration

Load the dataset:
```{r}
df = read.csv(file.path(data_path, "jobs.csv"))
```

Sample from the original dataset:
```{r, eval=F}
data = read.csv(file.path(data_path, "aug_train.csv"))
set.seed(020198)
sample = sample(1:nrow(data), 5000)
df = data[sample,]
write.csv(df, file.path(data_path, "jobs.csv"), row.names = FALSE)
```

Skim over it:
```{r, eval=F}
head(df)
summary(df)
str(df)
```

Convert data types to the proper format:
```{r}
df = df %>%
  mutate(across(where(is.character), ~ na_if(., ""))) %>%
  mutate(across(where(is.character) | matches("target"), ~ as.factor(.)))
```

Detail of factors:
```{r, eval=F}
df %>%
  select(., where(is.factor)) %>%
  sapply(., table)
table(df$last_new_job)
```

Collapse categories for factor Experience:
- Entry-level: 0-2
- Junior-level: 3-4
- Mid-level: 5-6
- Senior-level: 7-10
- Chief: 11 - INF

Collapse categories for factor Company Size:
- Small
-Medium
-Large

```{r, eval=F}
entry_level = c('<1','1','2')
junior_level = c('3','4')
mid_level = c('5','6')
senior_level = c('7','8','9','10')
chief_level = c('11','12','13','14','15','16','17','18','19','20','>20')

small_company = c('<10', '10/49')
medium_company = c('50-99','100-500','500-999')
large_company = c('1000-4999','5000-9999','10000+')

df = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(experience = case_when(experience %in% entry_level ~ "Entry Level",
                                experience %in% junior_level ~ "Junior Level",
                                experience %in% mid_level ~ "Mid Level",
                                experience %in% senior_level ~ "Senior Level",
                                experience %in% chief_level ~ "Chief Level",
                                TRUE ~ experience)) %>%
  mutate(company_size = case_when(company_size %in% small_company ~ 'Small Company',
                                  company_size %in% medium_company ~ 'Medium Company',
                                  company_size %in% large_company ~ 'Large Company',
                                  TRUE ~ company_size)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))
```
 
### Missing Values

Count number of missing values across each row:
```{r}
count_na = function(x) {sum(is.na(x))}
df = df %>%
  mutate(across(matches("company"), ~ as.character(.))) %>%
  mutate(across(matches("company"), ~ na_if(., "NA"))) %>%
  mutate(across(matches("company"), ~ as.factor(.))) %>%
  mutate(count_na = apply(., 1, count_na))
summary(df$count_na)
boxplot(df$count_na)
table(df$count_na)

df = df %>%
  filter(., count_na < 5) %>%
  select(., -c("count_na"))
```

For NA's:
- If education level is null and they are in university -> highschool ok
- If major_discipline != NA then education_level graduate at least ok
- If graduated/masters then impute major discipline, else no major discipline ok
- If experience is NA and last_new_job, company_type, company_size != 0 then impute, else delete
- If gender is missing impute with other
- If company information is missing impute with unknown.

Logical imputation:
```{r}
df = df %>%
  mutate(f.enrolled = case_when(enrolled_university == "no_enrollment" ~ "No",
                                !is.na(enrolled_university) ~ "Yes"))
df = df %>%
  # Convert factors to strings in order to impute them
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  
  # Impute education level as mentioned above
  mutate(education_level = case_when(is.na(education_level) & f.enrolled == "Yes" ~ "High School",
                                     !is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "Graduate",
                                     TRUE ~ education_level)) %>%
  
  # Impute major_discipline as mentioned above
  mutate(major_discipline = case_when(is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "No Major",
                                      is.na(major_discipline) & education_level %in% c("Graduate", "Masters", "Phd") ~ "Other",
                                      TRUE ~ major_discipline)) %>%
  
  # Impute enrolled_university
  mutate(enrolled_university = case_when(is.na(enrolled_university) & education_level %in% c("Masters", "Phd") ~ "no_enrollment",
                                         TRUE ~ enrolled_university)) %>%
  
  # Impute experience
  mutate(experience = case_when(is.na(experience) & (is.na(last_new_job) & is.na(company_size) & is.na(company_type)) ~ "<1",
                                TRUE ~ experience)) %>%
  
  # Impute gender
  mutate(gender = case_when(is.na(gender) ~ "Other",
                            TRUE ~ gender)) %>%
  
  # Impute company
  mutate(company_size = case_when(is.na(company_size) ~ "Unknown",
                                  TRUE ~ company_size)) %>%
  mutate(company_type = case_when(is.na(company_type) ~ "Other",
                                  TRUE ~ company_type)) %>%
  
  # Convert back to factors    
  mutate(across(where(is.character), ~ as.factor(.))) %>%
  
  # Drop unused columns
  select(., -c("f.enrolled"))
```

Class frequency:
```{r}
table(df$gender)
table(df$relevent_experience)
table(df$education_level) # Collapse PhD and Primary School?
table(df$major_discipline) # Collapse major_discipline?
table(df$experience)
table(df$company_type) # It is not other, it is unknown, what should we do
table(df$company_size) # Ask Lidia
table(df$last_new_job)
table(df$enrolled_university)
```

Remaining NA's depicted below. See how at most we have 2% of missing values on last_new_job:
```{r}
colSums(is.na(df))
```

Indicator of rows which still have NA's:
```{r}
imputed_indicator = function(x) {if(count_na(x)>0) {return(TRUE)} else {return(FALSE)}}

df = df %>%
  mutate(imputed = apply(., 1, imputed_indicator))
```

## FAMD IMPUTATION

Impute with FAMD method:
```{r}
res.famd = imputeFAMD(select(df, -c("target", "city", "enrollee_id", "imputed")))
```

Compare class frequency after imputation:
```{r}
prop.table(table(df$education_level))
prop.table(table(res.famd$completeObs$education_level))
prop.table(table(df$last_new_job))
prop.table(table(res.famd$completeObs$last_new_job))
prop.table(table(df$enrolled_university))
prop.table(table(res.famd$completeObs$enrolled_university))
summary(df$training_hours)
```

Store complete dataset:
```{r}
df = data.frame(res.famd$completeObs, select(df, c("target", "city", "enrollee_id", "imputed")))
```

Mutate strings after FAMD converted them into dummy:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(gender = str_remove(gender, "gender_")) %>%
  mutate(major_discipline = str_remove(major_discipline, "major_discipline_")) %>%
  mutate(company_type = str_remove(company_type, "company_type_")) %>%
  mutate(experience = str_remove(experience, "experience_")) %>%
  mutate(last_new_job = str_remove(last_new_job, "last_new_job_")) %>%
  mutate(across(where(is.character), ~ as.factor(.)))
```

Convert experience into a numerical variable:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(n.experience = case_when(experience == "<1" ~ "0",
                                experience == ">20" ~ "25",
                                TRUE ~ experience)) %>%
  mutate(n.experience = as.integer(n.experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))

summary(df$n.experience)
summary(df$experience)
```

Convert city development index into a categorical variable:
```{r}
groups = 5

df$f.city_development_index = as.ordered(cut2(df$city_development_index, g=groups, m=nrow(df)/groups))
table(df$f.city_development_index)
```

Since company size has a lot of NA's we do not convert it into numerical. Otherwise, the imputation would not make any sense.

Write the dataset:
```{r, eval=F}
write.csv(df, file.path(data_path, "jobs_compl.csv"), row.names = FALSE)
```

### Outlier treatment

Look at univariant outliers:
```{r, eval=F}
extreme_out = quantile(df$training_hours)[[4]]+3*IQR(df$training_hours)

ggplot(data = df, aes(x="", y=training_hours)) +
  geom_boxplot(width=0.5) +
  geom_hline(yintercept = extreme_out, color="red") +
  scale_y_continuous(labels=scales::comma) 
labs(title='Boxplot Training Hours',
     y="Training Hours") +
  # Do not show x axis
  theme(axis.text.x=element_blank(), axis.ticks.x = element_blank(), axis.line.x = element_blank(), axis.title.x=element_blank())

num_outliers = df %>%
  filter(., training_hours > extreme_out) %>%
  nrow()
num_outliers

outliers = df %>%
  filter(., training_hours > extreme_out)

prop.table(table(df$gender))
prop.table(table(outliers$gender))
prop.table(table(df$relevent_experience))
prop.table(table(outliers$relevent_experience))
prop.table(table(df$enrolled_university))
prop.table(table(outliers$enrolled_university))
prop.table(table(df$education_level))
prop.table(table(outliers$education_level))
prop.table(table(df$major_discipline))
prop.table(table(outliers$major_discipline))
prop.table(table(df$last_new_job))
prop.table(table(outliers$last_new_job))
```

```{r, eval=F}
summary(df$training_hours)
boxplot(df$training_hours)
```

# Modelization

```{r}
cat = FactoMineR::catdes(df[,-c(13:15)], 12)
cat$test.chi2
cat$quanti.var
cat$category
cat$quanti
```

INSPECT THE NULL MODEL
```{r}
df = select(df, -c("city", "enrollee_id", "imputed"))
m0 = glm(target ~ 1, data=df, family=binomial)
summary(m0)
```

HOW TO WORK WITH EXPERIENCE: We consider it as numeric and numeric with a third degree polynomial. The p-value suggests that the 3 degree in not useful, and that we can work with order 2 (we can draw that conclusion because the variables constructed by poly are orthogonal). Additionally, with a deviance test it can be observed that in fact the second order transformation results in a better model than without the transformation.
After that, this model is compared against a model using the categorical experience, and as they are not nested models we can not use the deviance test (anova), so AIC has been used. It can be seen that The polynomial model is better than the categorical.
However a questions arises here : does the LARGE number of categories in experience hinder the power of modeling it as a factor? A logical collapse was attempted and the model was statistically significant (p-value <0.05) and better than without collapsing. However, when compared against the numerical models, it was still worse than them. Hence, it was decided to work with the numerical of order 2.

Q: Should we use Anova() in any case? Due to the imbalance on the target (4:1).
```{r}
mnexp = glm(target ~ n.experience, data=df, family=binomial)
mnexppoly3 = glm(target ~ poly(n.experience,3), data=df, family=binomial)
summary(mnexppoly)
mnexppoly2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)
anova(mnexppoly2, mnexppoly3, test='Chisq')
anova(mnexp,mnexppoly2,test='Chisq')

mcexp = glm(target ~ experience, data=df, family=binomial)

summary(mnexpp)
anova(mnexp,mnexppoly,test='Chisq')
AIC(mcexp,mnexp,mnexppoly)
```

Let's look at collapsed experience in two different ways. Logical and by quartiles -> Logical is better:
```{r}
entry_level = c('<1','1','2')
junior_level = c('3','4')
mid_level = c('5','6')
senior_level = c('7','8','9','10')
chief_level = c('11','12','13','14','15','16','17','18','19','20','>20')

df = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(collapsed_exp = case_when(experience %in% entry_level ~ "Entry Level",
                                experience %in% junior_level ~ "Junior Level",
                                experience %in% mid_level ~ "Mid Level",
                                experience %in% senior_level ~ "Senior Level",
                                experience %in% chief_level ~ "Chief Level",
                                TRUE ~ experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))


groups = 5

df$collapsed_exp2 = as.ordered(cut2(df$n.experience, g=groups, m=nrow(df)/groups))
table(df$collapsed_exp)
table(df$collapsed_exp2)
```

Hereby we can observe how numerical experience improves more the model:
```{r}
mcexpcol = glm(target ~ collapsed_exp, data=df, family=binomial)
mcexpcol2 = glm(target ~ collapsed_exp2, data=df, family=binomial)

anova(mcexpcol, mcexpcol2, test='Chisq')

anova(mcexpcol,mcexp,test='Chisq')
anova(mcexpcol2,mcexp,test='Chisq')

AIC(mcexp,mcexpcol,mnexppoly,mnexp, mcexpcol2)
```

The same analysis can be done for the city development index -> discretized cdi without previous transformation is much better:
```{r}
mncdi = glm(target ~ city_development_index, data=df, family=binomial); summary(mncdi) # Numerical
mfcdi = glm(target ~ f.city_development_index, data=df, family=binomial); summary(mfcdi) # Categorical (collapsed above)

anova(mncdi, mfcdi, test='Chisq')
AIC(mncdi, mfcdi)

# For the improved cdi^-0.5 - known from the marginal model plots
mncdi_tr = glm(target ~ I(city_development_index^-0.5), data=df, family=binomial)
anova(mncdi_tr, mfcdi, test='Chisq')
AIC(mncdi_tr, mfcdi)

# Discretizing the transformed index
groups = 5

df$f.city_development_index_tr = as.ordered(cut2(df$city_development_index^-0.5, g=groups, m=nrow(df)/groups))
mfcdi_tr = glm(target ~ f.city_development_index_tr, data=df, family=binomial)
anova(mfcdi, mfcdi_tr, test='Chisq')
AIC(mfcdi, mfcdi_tr)
```

---
# This is just for the transformation of ^-0.5
Inspect the numerical variables:
```{r}
m1 = glm(target ~ training_hours, data=df, family=binomial)
m2 = glm(target ~ city_development_index, data=df, family=binomial)

m3 = glm(target ~ training_hours+city_development_index, data=df, family=binomial)
m4 = glm(target ~ training_hours*city_development_index, data=df, family=binomial)
```

The marginal model plots suggest a  ^(-1/2) transformation over the city development variable
```{r}
marginalModelPlots(m3)
```

As it can be seen, the transformation increases the fitness, and it results in a better model following AIC criterion. 
```{r}
m5 = glm(target ~ training_hours+I(city_development_index^-0.5), data=df, family=binomial)
marginalModelPlots(m5)
AIC(m3,m5)
```
---

Further inspect the numerical variables:
```{r}
m1 = glm(target ~ training_hours, data=df, family=binomial)
m2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)

m3 = glm(target ~ training_hours+poly(n.experience,2), data=df, family=binomial)
m4 = glm(target ~ training_hours*poly(n.experience,2), data=df, family=binomial)
```

```{r}
# Gross effects
anova(m0,m1,test="Chisq")
anova(m0,m2,test="Chisq")

# Net effects
anova(m1,m3,test="Chisq")
anova(m2,m3,test="Chisq")

# Interaction effects
anova(m3,m4,test="Chisq")

AIC(m0, m1, m2, m3, m4) # Either m2 or m3! 
```

The marginal model plots do not suggest any transformations:
```{r}
marginalModelPlots(m3) # Fits perfectly :)
```

Now, the additive effect of variables is explored by using the step function with AIC, to be more permissive.
```{r}
aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "city_development_index", "f.city_development_index_tr"))
```

```{r}
maux = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
m6 = step(maux)
vif(m6)
```

```{r}
anova(m4,m6,test="Chisq")
AIC(m4,m6)
```

To check for interactions, we use the step function again, but now with BIC criterion, to be more restrictive.
```{r}
m7 = step(maux , scope = . ~ .^2, k = log(nrow(df)))
AIC(m4, m7)
```

---
# BIC with n.cdi and f.cdi -> f.cdi much better

In the numerical there is interaction with company size, in the categorical not!
```{r}
aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "f.city_development_index", "f.city_development_index_tr"))
mn = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
mn_bic = step(mn , scope = . ~ .^2, k = log(nrow(df)))

aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "city_development_index", "f.city_development_index_tr"))
mf = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
mf_bic = step(mf , scope = . ~ .^2, k = log(nrow(df)))

AIC(mn_bic, mf_bic)
```

# Gross effects/BIC with company size uncollapsed and collapsed
```{r}
```
---

As we can obviously see, the results of the additive terms with BIC are a set of the ones with AIC, an one interaction is preserved. The merged model is as follows:
```{r}
m8 = glm(target ~ I(city_development_index^(-0.5))*company_size + relevent_experience + 
    education_level + experience + company_type + 
    last_new_job + training_hours, family = "binomial", data = df)

anova(m4,m8,test='Chisq')
anova(m7,m8,test='Chisq')
AIC(m4,m7,m8)
```

As we only got one relevant interaction with BIC criterion, and the two interaction terms are between the set of variables that BIC proposes as additive as well, it was decided to further explore if the interactions between the other BIC relevant improved the model.

As it can be seen, adding the interaction of company_size * education_level results in a significant improvement of the model. Adding however education_level*city_development results in a slightly different model with a very similar AIC value. However, adding this interaction to model 9, results in a statistically different and better model.
```{r}
m9 = glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours, family = "binomial", data = df)

m10 = glm(target ~ (company_size+ education_level)*I(city_development_index^(-0.5)) + relevent_experience + 
    experience + company_type + last_new_job + training_hours, family = "binomial", data = df)

m11 =  glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours + education_level*I(city_development_index^(-0.5)) , family = "binomial", data = df)

summary(m11)
anova(m8,m9,test='Chisq')
anova(m8,m10,test='Chisq')
anova(m9,m11,test='Chisq')

AIC(m8,m9,m10,m11)
```

ROC Curve:
```{r}
library(pROC)
prob=predict(mf_bic, type=c("response"))
df$prob=prob
g = roc(target ~ prob, data = df)
plot(g)
```

Area under the ROC Curve:
```{r}
auc(df$target, prob)
```

Model diagnostics:
```{r}
residuals_p = residuals(m7, type="pearson")
residuals_d = residuals(m7, type="deviance")
residuals_s = rstudent(m7)
hat = hatvalues(m7)
cook = cooks.distance(m7)
beta = dfbetas(m7)
outlierTest(m7)
residualPlots(m7)
marginalModelPlots(m7)
```

```{r}
hat_p = as.data.frame(cbind(hat, residuals_p))
hat_p_f = hat_p[which(hat_p$hat < quantile(hat, probs = seq(0, 1, 0.005))[200]),]
scatterplot(hat_p_f$hat, hat_p_f$residuals_p)
scatterplot(hat, residuals_p)

abline(h=3)
abline(h=5)
p = m11$rank
n = nrow(aux)
abline(v=2*p/n)
abline(v=3*p/n)
```

```{r}
Boxplot(cook)
abline(h=0.02)

aux = aux[which(hat < quantile(hat, probs = seq(0, 1, 0.1))[10]),]
aux = df[which(cook < 0.02),]
aux = aux[which(residuals_p < 5),]

m11 =  glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours + education_level*I(city_development_index^(-0.5)) , family = "binomial", data = aux)

prob=predict(m11, type=c("response"))
auc(aux$target, prob)

scatterplot(hat, residuals_p)
```


```{r}
# SUMMARY OF USEFUL PLOTS
avPlots(m1)
par(mfrow=c(2,2))
plot(m1) 
par(mfrow=c(1,1))
plot(allEffects(m11),ask=FALSE)
residualPlots(m7)
crPlots(m1)
marginalModelPlots(m1)
outlierTest(m1)
influencePlot(m7)
```

```{r}
eff <- allEffects(m7) 

plot(eff, main= FALSE,rug=FALSE,colors=1, band.colors=3, col=1, 
     xlim=c(0, 20), ylim = qlogis(c(0.1, 0.6))) # also log(c(.1, .6)/c(.6, .1))
```


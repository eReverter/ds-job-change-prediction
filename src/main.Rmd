---
title: "Assignment 2"
author: 'Enric Reverter & Gerard Pons'
date: "14/10/2021"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, eval=F}
knitr::opts_chunk$set(echo = TRUE)
```

Assumptions:
- Sample from incomplete dataset.
- Everyone in this dataset is currently working.


### Required libraries

```{r, eval=F}
## Data manipulation
library(tidyverse)
library(dplyr)
library(mice)
library(Hmisc)
## Statistics
library(lsr)
library(missMDA)
library(VIM)
library(chemometrics)
library(arules)
library(skimr)
library(car)
library(FactoMineR)
library(factoextra)
library(effects)
## Plots
library(ggplot2)
library(ggExtra)
library(ggthemes)
library(processx)
library(plotly)
library(cowplot)
library(gridExtra)
library(RColorBrewer)
theme_set(theme_bw())
## Set data path
setwd("..")
data_path = file.path(getwd(), "data")
plot_path = file.path(getwd(), "plots")
```


# Data Exploration

Load the dataset:
```{r}
df = read.csv(file.path(data_path, "jobs.csv"))
```

Sample from the original dataset:
```{r, eval=F}
data = read.csv(file.path(data_path, "aug_train.csv"))
set.seed(020198)
sample = sample(1:nrow(data), 5000)
df = data[sample,]
write.csv(df, file.path(data_path, "jobs.csv"), row.names = FALSE)
```

Skim over it:
```{r, eval=F}
head(df)
summary(df)
str(df)
```

Convert data types to the proper format:
```{r}
df = df %>%
  mutate(across(where(is.character), ~ na_if(., ""))) %>%
  mutate(across(where(is.character) | matches("target"), ~ as.factor(.)))
```

Detail of factors:
```{r, eval=F}
df %>%
  select(., where(is.factor)) %>%
  sapply(., table)
table(df$last_new_job)
```

REMOVE
Collapse categories for factor Experience:
- Entry-level: 0-2
- Junior-level: 3-4
- Mid-level: 5-6
- Senior-level: 7-10
- Chief: 11 - INF

Collapse categories for factor Company Size:
- Small
-Medium
-Large

```{r, eval=F}
#REMOVE
entry_level = c('<1','1','2')
junior_level = c('3','4')
mid_level = c('5','6')
senior_level = c('7','8','9','10')
chief_level = c('11','12','13','14','15','16','17','18','19','20','>20')

small_company = c('<10', '10/49')
medium_company = c('50-99','100-500','500-999')
large_company = c('1000-4999','5000-9999','10000+')

df = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(experience = case_when(experience %in% entry_level ~ "Entry Level",
                                experience %in% junior_level ~ "Junior Level",
                                experience %in% mid_level ~ "Mid Level",
                                experience %in% senior_level ~ "Senior Level",
                                experience %in% chief_level ~ "Chief Level",
                                TRUE ~ experience)) %>%
  mutate(company_size = case_when(company_size %in% small_company ~ 'Small Company',
                                  company_size %in% medium_company ~ 'Medium Company',
                                  company_size %in% large_company ~ 'Large Company',
                                  TRUE ~ company_size)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))
```
 
### Missing Values

As it can be seen, the dataset contains a lot of missing values, in some cases even exceeding the 30% of values from an attribute. These high values conditioned the imputation methods, which was first done using logical methods. Moreover, there are observations with more than 50% of the variables as null, which have been decided to be eliminated.
```{r}
count_na = function(x) {sum(is.na(x))}
df = df %>%
  mutate(across(matches("company"), ~ as.character(.))) %>%
  mutate(across(matches("company"), ~ na_if(., "NA"))) %>%
  mutate(across(matches("company"), ~ as.factor(.))) %>%
  mutate(count_na = apply(., 1, count_na))
summary(df$count_na)
boxplot(df$count_na)
table(df$count_na)

df = df %>%
  filter(., count_na < 5) %>%
  select(., -c("count_na"))
```

The rules used for logically imputation where as follow:

- If the education level is null but they are enrolled in university, the education is set to highschool.
- If major discipline is not null, the education level should be at least graduate.
- If company information is missing, it is imputed with Unknown, as the number of missing values for company information exceeds 30%.
- If gender is missing, it is imputed with Unknown, as there are nearly 30% of missing values in gender.
- If Major_Discipline is null, it is imputed with Other if the education level is Graduate, Masters or PhD, and imputed to No Major otherwise.
- If Experience ,Last_New_Job and Company information are null, the experience is imputed to <1.

```{r}
df = df %>%
  mutate(f.enrolled = case_when(enrolled_university == "no_enrollment" ~ "No",
                                !is.na(enrolled_university) ~ "Yes"))
df = df %>%
  # Convert factors to strings in order to impute them
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  
  # Impute education level as mentioned above
  mutate(education_level = case_when(is.na(education_level) & f.enrolled == "Yes" ~ "High School",
                                     !is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "Graduate",
                                     TRUE ~ education_level)) %>%
  
  # Impute major_discipline as mentioned above
  mutate(major_discipline = case_when(is.na(major_discipline) & !(education_level %in% c("Graduate", "Masters", "Phd")) ~ "No Major",
                                      is.na(major_discipline) & education_level %in% c("Graduate", "Masters", "Phd") ~ "Other",
                                      TRUE ~ major_discipline)) %>%
  
  # Impute enrolled_university
  mutate(enrolled_university = case_when(is.na(enrolled_university) & education_level %in% c("Masters", "Phd") ~ "no_enrollment",
                                         TRUE ~ enrolled_university)) %>%
  
  # Impute experience
  mutate(experience = case_when(is.na(experience) & (is.na(last_new_job) & is.na(company_size) & is.na(company_type)) ~ "<1",
                                TRUE ~ experience)) %>%
  
  # Impute gender
  mutate(gender = case_when(is.na(gender) ~ "Other",
                            TRUE ~ gender)) %>%
  
  # Impute company
  mutate(company_size = case_when(is.na(company_size) ~ "Unknown",
                                  TRUE ~ company_size)) %>%
  mutate(company_type = case_when(is.na(company_type) ~ "Other",
                                  TRUE ~ company_type)) %>%
  
  # Convert back to factors    
  mutate(across(where(is.character), ~ as.factor(.))) %>%
  
  # Drop unused columns
  select(., -c("f.enrolled"))
```
REMOVE
Class frequency:
```{r}
#REMOVE
table(df$gender)
table(df$relevent_experience)
table(df$education_level) # Collapse PhD and Primary School?
table(df$major_discipline) # Collapse major_discipline?
table(df$experience)
table(df$company_type) # It is not other, it is unknown, what should we do
table(df$company_size) # Ask Lidia
table(df$last_new_job)
table(df$enrolled_university)
```

After the logical imputation, the NA values do not account for more than 2% in any of the categories, and it was decided to impute them with factorial analysis for mixed data. It must be noted that a new flag attribute ‘Imputed’ was created, in order to keep track of these imputed observations when modelling, as they could cause problems.

Indicator of rows which still have NA's:
```{r}
colSums(is.na(df))

imputed_indicator = function(x) {if(count_na(x)>0) {return(TRUE)} else {return(FALSE)}}

df = df %>%
  mutate(imputed = apply(., 1, imputed_indicator))
```

## FAMD IMPUTATION

Impute with FAMD method:
```{r}
res.famd = imputeFAMD(select(df, -c("target", "city", "enrollee_id", "imputed")))
```

As it can be seen, the class frequencies after imputation have been compared to the ones before it, and there is no notable change.
```{r}
prop.table(table(df$education_level))
prop.table(table(res.famd$completeObs$education_level))
prop.table(table(df$last_new_job))
prop.table(table(res.famd$completeObs$last_new_job))
prop.table(table(df$enrolled_university))
prop.table(table(res.famd$completeObs$enrolled_university))
summary(df$training_hours)
```

Store complete dataset:
```{r}
df = data.frame(res.famd$completeObs, select(df, c("target", "city", "enrollee_id", "imputed")))
```

Mutate strings after FAMD converted them into dummy:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(gender = str_remove(gender, "gender_")) %>%
  mutate(major_discipline = str_remove(major_discipline, "major_discipline_")) %>%
  mutate(company_type = str_remove(company_type, "company_type_")) %>%
  mutate(experience = str_remove(experience, "experience_")) %>%
  mutate(last_new_job = str_remove(last_new_job, "last_new_job_")) %>%
  mutate(across(where(is.character), ~ as.factor(.)))
```
With the complete dataset, some new attributes have been created: a new numerical variable has been created from the factor experience and a new factor has been created from the variable of city development. In future steps, it will be decided which one is the most suitable for the modelling process. It must be noted that since company size had a lot of NA, it has not been converted into numerical.

Convert experience into a numerical variable:
```{r}
df = df %>%
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(n.experience = case_when(experience == "<1" ~ "0",
                                experience == ">20" ~ "25",
                                TRUE ~ experience)) %>%
  mutate(n.experience = as.integer(n.experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))

summary(df$n.experience)
summary(df$experience)
```

Convert city development index into a categorical variable:
```{r}
groups = 5

df$f.city_development_index = as.ordered(cut2(df$city_development_index, g=groups, m=nrow(df)/groups))
table(df$f.city_development_index)
```

Write the dataset:
```{r, eval=F}
write.csv(df, file.path(data_path, "jobs_compl.csv"), row.names = FALSE)
```

### Outlier treatment

Univariate outliers can not be seen in the dataset for the two numerical variables present. One could think that training_hours contains some outliers, as they are above the extreme threshold. However, they are not too extreme and all of them have a very plausible value, hence imputation would not be a good practice in this case.

```{r, eval=F}
extreme_out = quantile(df$training_hours)[[4]]+3*IQR(df$training_hours)

ggplot(data = df, aes(x="", y=training_hours)) +
  geom_boxplot(width=0.5) +
  geom_hline(yintercept = extreme_out, color="red") +
  scale_y_continuous(labels=scales::comma) 
labs(title='Boxplot Training Hours',
     y="Training Hours") +
  # Do not show x axis
  theme(axis.text.x=element_blank(), axis.ticks.x = element_blank(), axis.line.x = element_blank(), axis.title.x=element_blank())

num_outliers = df %>%
  filter(., training_hours > extreme_out) %>%
  nrow()
num_outliers

outliers = df %>%
  filter(., training_hours > extreme_out)

prop.table(table(df$gender))
prop.table(table(outliers$gender))
prop.table(table(df$relevent_experience))
prop.table(table(outliers$relevent_experience))
prop.table(table(df$enrolled_university))
prop.table(table(outliers$enrolled_university))
prop.table(table(df$education_level))
prop.table(table(outliers$education_level))
prop.table(table(df$major_discipline))
prop.table(table(outliers$major_discipline))
prop.table(table(df$last_new_job))
prop.table(table(outliers$last_new_job))
```

```{r, eval=F}
#REMOVE
summary(df$training_hours)
boxplot(df$training_hours)
```

# Modelization

Before starting with the model, it is interesting to describe the response variable. It can be seen that it is significantly associated with all the numerical and categorical variables, except for training_hours, which sits really close to the threshold. It is also worth noting that the variables which have been kept in purpose both in numerical and categorical form are the ones that have a more significant association, meaning that the future assessment of how to treat them will be of particular interest. Overall, what can be said is that in general people who want to change jobs tend to be from less developed cities, have less experience and lower education.


```{r}
cat = FactoMineR::catdes(df[,-c(13:15)], 12)
cat$test.chi2
cat$quanti.var
cat$category
cat$quanti
```

Before starting with the modelling, the data should be split into working and test datasets, so that the created model can be compared and assessed with data that it has not seen, hence limiting overfitting.
```{r}
train = ''
test = ''
```


INSPECT THE NULL MODEL
```{r}
df = select(df, -c("city", "enrollee_id", "imputed"))
m0 = glm(target ~ 1, data=df, family=binomial)
summary(m0)
```

After computing the null model, it was assessed how to treat the attribute experience, as a factor or as a numerical variable. 

Regarding being numerical, polynomial transformations were applied to it. It was seen that the p-value for a third degree polynomial suggests that this transformation is not needed (this conclusion can only be drawn because the variables constructed by Poly function are orthogonal), hence only a second order polynomial was kept. Using deviance tests, the comparison with the normal variable and the transformed one yield significantly different models, and with a better performance for the transformed one.


Q: Should we use Anova() in any case? Due to the imbalance on the target (4:1).
```{r}
mnexp = glm(target ~ n.experience, data=df, family=binomial)
summary(mnexp)
mnexppoly3 = glm(target ~ poly(n.experience,3), data=df, family=binomial)
summary(mnexppoly3)
mnexppoly2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)
summary(mnexppoly2)
anova(mnexp,mnexppoly2,test='Chisq')

```

Regarding treating it as a factor, as it has more than 20 categories, some collapsations have been found to improve the model results:
-Collapsing by quantiles
-Collapsing the model logically in Entry Level, Junior Level, Mid Level, Senior Level and Chief Level, using some well defined year ranges for the Data Science field.

```{r}
entry_level = c('<1','1','2')
junior_level = c('3','4')
mid_level = c('5','6')
senior_level = c('7','8','9','10')
chief_level = c('11','12','13','14','15','16','17','18','19','20','>20')

df = df %>% 
  mutate(across(where(is.factor), ~ as.character(.))) %>%
  mutate(collapsed_exp = case_when(experience %in% entry_level ~ "Entry Level",
                                experience %in% junior_level ~ "Junior Level",
                                experience %in% mid_level ~ "Mid Level",
                                experience %in% senior_level ~ "Senior Level",
                                experience %in% chief_level ~ "Chief Level",
                                TRUE ~ experience)) %>%
  mutate(across(where(is.character), ~ as.factor(.)))


groups = 5

df$collapsed_exp2 = as.ordered(cut2(df$n.experience, g=groups, m=nrow(df)/groups))
table(df$collapsed_exp)
table(df$collapsed_exp2)
```
Comparing both collapsed models, the logically collapsed results in the best one.

```{r} 
mcexp = glm(target ~ experience, data=df, family=binomial)
summary(mcexp)

mcexpcol = glm(target ~ collapsed_exp, data=df, family=binomial)
mcexpcol2 = glm(target ~ collapsed_exp2, data=df, family=binomial)


anova(mcexpcol2,mcexp,test='Chisq')
anova(mcexpcol, mcexpcol2, test='Chisq')
anova(mcexpcol,mcexp,test='Chisq')

```

After getting the best numerical and categorical transformations for the variable, the models created with them were compared. As they are not nested models, the deviance test anova() can not be applied, and it was decided to use AIC instead. It can be clearly seen that the numerical treatment of the variable outperforms the categorical, hence is the one that will be used in the following models. 

```{r}
AIC(mcexp,mcexpcol,mnexppoly,mnexp, mcexpcol2)
```

The same analysis can be done for the city development index (which will not be extensively reported). Even after performing the transformation suggested by the MarginalModelPlots, the discretized version of the city development index is much better.

```{r}
mncdi = glm(target ~ city_development_index, data=df, family=binomial); summary(mncdi) # Numerical
mfcdi = glm(target ~ f.city_development_index, data=df, family=binomial); summary(mfcdi) # Categorical (collapsed above)

anova(mncdi, mfcdi, test='Chisq')
AIC(mncdi, mfcdi)

# For the improved cdi^-0.5 - known from the marginal model plots
marginalModelPlots(mncdi)
mncdi_tr = glm(target ~ I(city_development_index^-0.5), data=df, family=binomial)
marginalModelPlots(mncdi_tr)
anova(mncdi_tr, mfcdi, test='Chisq')
AIC(mncdi_tr, mfcdi)

# Discretizing the transformed index
groups = 5

df$f.city_development_index_tr = as.ordered(cut2(df$city_development_index^-0.5, g=groups, m=nrow(df)/groups))
mfcdi_tr = glm(target ~ f.city_development_index_tr, data=df, family=binomial)
anova(mfcdi, mfcdi_tr, test='Chisq')
AIC(mfcdi, mfcdi_tr)
```



---
REMOVE
#INTEGRATE THIS SOMEHOW 
# This is just for the transformation of ^-0.5
Inspect the numerical variables:
```{r}
m1 = glm(target ~ training_hours, data=df, family=binomial)
m2 = glm(target ~ city_development_index, data=df, family=binomial)

m3 = glm(target ~ training_hours+city_development_index, data=df, family=binomial)
m4 = glm(target ~ training_hours*city_development_index, data=df, family=binomial)
```
REMOVE
The marginal model plots suggest a  ^(-1/2) transformation over the city development variable
```{r}
marginalModelPlots(m3)
```
REMOVE
As it can be seen, the transformation increases the fitness, and it results in a better model following AIC criterion. 
```{r}
m5 = glm(target ~ training_hours+I(city_development_index^-0.5), data=df, family=binomial)
marginalModelPlots(m5)
AIC(m3,m5)
```
---

After having chosen the best variables to work with, the focus was on the two numerical variables, whose models were compared with and without interactions. As it can be seen with the deviance test, adding training hours to the model, either as an interaction or addition, does not yield a statistically different model, hence the second order transformation is kept.


```{r}
m1 = glm(target ~ training_hours, data=df, family=binomial)
m2 = glm(target ~ poly(n.experience,2), data=df, family=binomial)

m3 = glm(target ~ training_hours+poly(n.experience,2), data=df, family=binomial)
m4 = glm(target ~ training_hours*poly(n.experience,2), data=df, family=binomial)
```

```{r}
# Gross effects
anova(m0,m1,test="Chisq")
anova(m0,m2,test="Chisq")

# Net effects
anova(m1,m3,test="Chisq")
anova(m2,m3,test="Chisq")

# Interaction effects
anova(m3,m4,test="Chisq")

AIC(m0, m1, m2, m3, m4) # Either m2 or m3! GERARD: per anova-> m2 
```

Assessing it with marginal model plots, no transformations are suggested, as it yields a perfect fit.
```{r}
marginalModelPlots(m2) 
```

After that, the additive effect of variables is explored by using a step function with AIC, to be more permissive. It results in suggesting the addition of 7 of the variables to the model, which is significantly different and much better than the previous best one. Also, multicollinearity was discarded by doing a vif test.

```{r}
aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "city_development_index", "f.city_development_index_tr"))
maux = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
m5 = step(maux)
vif(m5)
anova(m2,m5,test="Chisq")
AIC(m2,m5)
```

To check for interactions the step function was used again, but this time using the BIC criterion,  in order to be more restrictive. The BIC criterion removed all of them, hence the model to work with is the one after the additive step.
```{r}
m6 = step(maux , scope = . ~ .^2, k = log(nrow(df)))
AIC(m2, m6)
```

--------------------------------------------------------------------------------------------------------
---
REMOVE
# BIC with n.cdi and f.cdi -> f.cdi much better

In the numerical there is interaction with company size, in the categorical not!
```{r}
aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "f.city_development_index", "f.city_development_index_tr"))
mn = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
mn_bic = step(mn , scope = . ~ .^2, k = log(nrow(df)))

aux = select(df, -c("experience", "collapsed_exp", "collapsed_exp2", "city_development_index", "f.city_development_index_tr"))
mf = glm(target ~ poly(n.experience,2) + ., data=aux, family=binomial)
mf_bic = step(mf , scope = . ~ .^2, k = log(nrow(df)))

AIC(mn_bic, mf_bic)
```

# Gross effects/BIC with company size uncollapsed and collapsed
```{r}
```
---
REMOVE
As we can obviously see, the results of the additive terms with BIC are a set of the ones with AIC, an one interaction is preserved. The merged model is as follows:
```{r}
#REMOVE
m8 = glm(target ~ I(city_development_index^(-0.5))*company_size + relevent_experience + 
    education_level + experience + company_type + 
    last_new_job + training_hours, family = "binomial", data = df)

anova(m4,m8,test='Chisq')
anova(m7,m8,test='Chisq')
AIC(m4,m7,m8)
```
REMOVE
As we only got one relevant interaction with BIC criterion, and the two interaction terms are between the set of variables that BIC proposes as additive as well, it was decided to further explore if the interactions between the other BIC relevant improved the model.

As it can be seen, adding the interaction of company_size * education_level results in a significant improvement of the model. Adding however education_level*city_development results in a slightly different model with a very similar AIC value. However, adding this interaction to model 9, results in a statistically different and better model.
```{r}
#REMOVE
m9 = glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours, family = "binomial", data = df)

m10 = glm(target ~ (company_size+ education_level)*I(city_development_index^(-0.5)) + relevent_experience + 
    experience + company_type + last_new_job + training_hours, family = "binomial", data = df)

m11 =  glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours + education_level*I(city_development_index^(-0.5)) , family = "binomial", data = df)

summary(m11)
anova(m8,m9,test='Chisq')
anova(m8,m10,test='Chisq')
anova(m9,m11,test='Chisq')

AIC(m8,m9,m10,m11)
```

ROC Curve:
```{r}
library(pROC)
prob=predict(mf_bic, type=c("response"))
df$prob=prob
g = roc(target ~ prob, data = df)
plot(g)
```

Area under the ROC Curve:
```{r}
auc(df$target, prob)
```

Model diagnostics:
```{r}
residuals_p = residuals(m7, type="pearson")
residuals_d = residuals(m7, type="deviance")
residuals_s = rstudent(m7)
hat = hatvalues(m7)
cook = cooks.distance(m7)
beta = dfbetas(m7)
outlierTest(m7)
residualPlots(m7)
marginalModelPlots(m7)
```

```{r}
hat_p = as.data.frame(cbind(hat, residuals_p))
hat_p_f = hat_p[which(hat_p$hat < quantile(hat, probs = seq(0, 1, 0.005))[200]),]
scatterplot(hat_p_f$hat, hat_p_f$residuals_p)
scatterplot(hat, residuals_p)

abline(h=3)
abline(h=5)
p = m11$rank
n = nrow(aux)
abline(v=2*p/n)
abline(v=3*p/n)
```

```{r}
Boxplot(cook)
abline(h=0.02)

aux = aux[which(hat < quantile(hat, probs = seq(0, 1, 0.1))[10]),]
aux = df[which(cook < 0.02),]
aux = aux[which(residuals_p < 5),]

m11 =  glm(target ~ (I(city_development_index^(-0.5)) + education_level)*company_size + relevent_experience + 
    experience + company_type + last_new_job + training_hours + education_level*I(city_development_index^(-0.5)) , family = "binomial", data = aux)

prob=predict(m11, type=c("response"))
auc(aux$target, prob)

scatterplot(hat, residuals_p)
```


```{r}
# SUMMARY OF USEFUL PLOTS
avPlots(m1)
par(mfrow=c(2,2))
plot(m1) 
par(mfrow=c(1,1))
plot(allEffects(m11),ask=FALSE)
residualPlots(m7)
crPlots(m1)
marginalModelPlots(m1)
outlierTest(m1)
influencePlot(m7)
```

```{r}
eff <- allEffects(m7) 

plot(eff, main= FALSE,rug=FALSE,colors=1, band.colors=3, col=1, 
     xlim=c(0, 20), ylim = qlogis(c(0.1, 0.6))) # also log(c(.1, .6)/c(.6, .1))
```

